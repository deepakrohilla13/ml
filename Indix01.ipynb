{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We will evaluate all models on \"Test\" data only which will not be the part of Training dataset.\n",
    "\n",
    "# In this submission we will use TF-IDF (Term frequency - Inverse document frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from IPython.core.display import display, HTML\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "from time import time\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>leaf</th>\n",
       "      <th>name</th>\n",
       "      <th>path</th>\n",
       "      <th>pathWithIds</th>\n",
       "      <th>depth</th>\n",
       "      <th>duplicates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15999</td>\n",
       "      <td>1.0</td>\n",
       "      <td>internal cd drives</td>\n",
       "      <td>computers &amp; accessories &gt; computer components ...</td>\n",
       "      <td>10161 &gt; 15997 &gt; 15998 &gt; 15999</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16004</td>\n",
       "      <td>1.0</td>\n",
       "      <td>boys</td>\n",
       "      <td>sports &amp; outdoors &gt; fan shop &gt; clothing &amp; acce...</td>\n",
       "      <td>10162 &gt; 16000 &gt; 16001 &gt; 16002 &gt; 16003 &gt; 16004</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16008</td>\n",
       "      <td>1.0</td>\n",
       "      <td>boots</td>\n",
       "      <td>automotive &gt; motorcycle &amp; powersports &gt; protec...</td>\n",
       "      <td>10163 &gt; 16005 &gt; 16006 &gt; 16007 &gt; 16008</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16010</td>\n",
       "      <td>1.0</td>\n",
       "      <td>kids bikes &amp; accessories</td>\n",
       "      <td>sports &amp; outdoors &gt; cycling &gt; kids bikes &amp; acc...</td>\n",
       "      <td>10162 &gt; 16009 &gt; 16010</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16013</td>\n",
       "      <td>1.0</td>\n",
       "      <td>bells &amp; sleigh bells</td>\n",
       "      <td>home &amp; kitchen &gt; seasonal décor &gt; ornaments &gt; ...</td>\n",
       "      <td>10164 &gt; 16011 &gt; 16012 &gt; 16013</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  leaf                      name  \\\n",
       "0  15999   1.0        internal cd drives   \n",
       "1  16004   1.0                      boys   \n",
       "2  16008   1.0                     boots   \n",
       "3  16010   1.0  kids bikes & accessories   \n",
       "4  16013   1.0      bells & sleigh bells   \n",
       "\n",
       "                                                path  \\\n",
       "0  computers & accessories > computer components ...   \n",
       "1  sports & outdoors > fan shop > clothing & acce...   \n",
       "2  automotive > motorcycle & powersports > protec...   \n",
       "3  sports & outdoors > cycling > kids bikes & acc...   \n",
       "4  home & kitchen > seasonal décor > ornaments > ...   \n",
       "\n",
       "                                     pathWithIds  depth  duplicates  \n",
       "0                  10161 > 15997 > 15998 > 15999    4.0           0  \n",
       "1  10162 > 16000 > 16001 > 16002 > 16003 > 16004    6.0           1  \n",
       "2          10163 > 16005 > 16006 > 16007 > 16008    5.0           1  \n",
       "3                          10162 > 16009 > 16010    3.0           0  \n",
       "4                  10164 > 16011 > 16012 > 16013    4.0           0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = defaultdict(LabelEncoder)\n",
    "\n",
    "category_json_path = 'categories.json'\n",
    "cat_file = open(category_json_path).read()\n",
    "cat_js = json.loads(cat_file)\n",
    "cat_dict_by_id = {}\n",
    "\n",
    "for x in cat_js:\n",
    "    cat_dict_by_id[x['id']] = x\n",
    "    \n",
    "all_leaf_list = []\n",
    "for x in list(filter(lambda x: x['leaf'] == True, cat_js)):\n",
    "    all_leaf_list.append(x)\n",
    "    \n",
    "df = pd.read_json(cat_file)\n",
    "df['depth'] = None\n",
    "df.loc[:,['depth']] = (df.apply(lambda row: float(len(row['path'].split('>'))), axis=1)).values\n",
    "\n",
    "\n",
    "df['name'] = df['name'].str.lower()\n",
    "df['path'] = df['path'].str.lower()\n",
    "names_count_df = df.groupby(['name']).agg(['count'])\n",
    "names_duplicate_df = names_count_df[(names_count_df['id']['count'] > 1)]\n",
    "df['duplicates'] = False\n",
    "df['duplicates'] = np.where(df['name'].isin(names_duplicate_df.index) & df['leaf'] == True, 1, 0)\n",
    "\n",
    "vocabulary = set()\n",
    "for idx, row in df.iterrows():\n",
    "    [vocabulary.update([path_seq.strip()]) for path_seq in row.path.split('>')]\n",
    "\n",
    "df = df[df.leaf == 1]\n",
    "\n",
    "df['leaf'] = np.where(df['leaf'] == True, 1.0, 0.0)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering:\n",
    "1. Using <span>TF-IDF<span> to introduce synthetic features\n",
    "2. TF-IDF has been calculated on entire dataset.\n",
    "3. TF-IDF has been applied to only dataset of our interest i.e. data subset having leaf nodes.\n",
    "4. Extracted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = df.apply(lambda row: \"\".join(row['path'].split('>')), axis = 1)\n",
    "vectorizer = TfidfVectorizer()\n",
    "x = vectorizer.fit_transform(x)\n",
    "df_tfidf = pd.DataFrame(x.todense())\n",
    "y = df.duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle and split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_tfidf, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def classify(clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    pred = clf.predict(X_test)\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    print(\"accuracy:   %0.3f\" % score)\n",
    "    clf_descr = str(clf).split('(')[0]\n",
    "    return str(clf), clf_descr, score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying various classifiers to pick best classifer while evaluating model on test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.793\n",
      "accuracy:   0.678\n",
      "accuracy:   0.764\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for clf, name in (\n",
    "        (GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True), \"GaussianProcessClassifier\"),\n",
    "        (GaussianNB(), \"GaussianNB\"),\n",
    "        (linear_model.LogisticRegression(), \"linear_model.LogisticRegression\")):\n",
    "    results.append(classify(clf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From above results we should select the LogisticRegression even the accuracy is slightly higer with GaussianProcessClassifier as it converged much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scoring_a(y_true, y_predict):\n",
    "    return metrics.accuracy_score(y_true, y_predict)\n",
    "\n",
    "clf = linear_model.LogisticRegression()\n",
    "scorer = make_scorer(scoring_a)\n",
    "cv_sets = ShuffleSplit(X_train.shape[0], n_iter = 10, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying GridSearchCV with LogisticRegression as with only 'LibLinear' kernel. (other kernels will not work with l1 penalty, we will try them later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning time in seconds: 100.39824295\n",
      "Best Accuracy with Test data: 0.763791763792\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=90, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=4242, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=True)\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "                'penalty':(\"l1\",\"l2\"),\n",
    "                'C':(1.0,0.9,0.8),\n",
    "                'class_weight':('balanced',None), \n",
    "                'max_iter':(90,100,110),\n",
    "                'solver':['liblinear'],\n",
    "                'warm_start':(True,False),\n",
    "                #'dual':(True,False),\n",
    "                'random_state':[4242]\n",
    "            }\n",
    "\n",
    "\n",
    "grid_obj = GridSearchCV(clf, parameters, cv = cv_sets)#, scoring = scorer)\n",
    "start = time()\n",
    "grid_fit = grid_obj.fit(X_train, y_train)\n",
    "end = time()\n",
    "delta = end-start\n",
    "best_clf = grid_fit.best_estimator_\n",
    "print(\"Learning time in seconds: \"+str(delta))\n",
    "print(\"Best Accuracy with Test data: \"+str(scoring_a(best_clf.predict(X_test),y_test)))\n",
    "print(best_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying other kernels with GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy: 0.763403263403\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=4242, solver='newton-cg', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "                #'penalty':[\"l2\"],\n",
    "                #'C':(1.0,0.9,0.8),\n",
    "                #'class_weight':('balanced',None), \n",
    "                #'max_iter':(90,100,110),\n",
    "                'solver':['liblinear','newton-cg','lbfgs','sag'],\n",
    "                #'warm_start':(True,False),\n",
    "                #'dual':(True,False),\n",
    "                'random_state':[4242]\n",
    "            }\n",
    "grid_obj_outher_solver_for_l2 = GridSearchCV(clf, parameters, cv = cv_sets)#, scoring = scorer)\n",
    "grid_fit_outher_solver_for_l2 = grid_obj_outher_solver_for_l2.fit(X_train, y_train)\n",
    "best_clf_outher_solver_for_l2 = grid_fit_outher_solver_for_l2.best_estimator_\n",
    "print(\"Best Accuracy: \"+str(scoring_a(best_clf_outher_solver_for_l2.predict(X_test),y_test)))\n",
    "print(best_clf_outher_solver_for_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Learning time with best settings found by GridSearchCV in <span style=\"background-color:yellow;\">seconds: 0.0641689300537</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Accuracy with best settings found by GridSearchCV on Test data:  <span style=\"background-color:yellow;\"> 76.3791763792 % </span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_clf = linear_model.LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=90, multi_class='ovr', n_jobs=1,\n",
    "          penalty='l2', random_state=4242, solver='liblinear', tol=0.0001,\n",
    "          verbose=0, warm_start=True)\n",
    "start = time()\n",
    "lr_clf.fit(X_train, y_train)\n",
    "end = time()\n",
    "delta = end-start\n",
    "pred = lr_clf.predict(X_test)\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "display(HTML('Learning time with best settings found by GridSearchCV in <span style=\"background-color:yellow;\">seconds: '+str(delta)+'</span>'))\n",
    "display(HTML('Accuracy with best settings found by GridSearchCV on Test data:  <span style=\"background-color:yellow;\"> '+str(score*100)+' % </span>'))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
